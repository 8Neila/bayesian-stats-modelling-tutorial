{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter estimation and hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "from ipywidgets import interact\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives of Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understand what priors, likelihoods and posteriors are;\n",
    "2. Use random sampling for parameter estimation to appreciate the relationship between sample size & the posterior distribution, along with the effect of the prior;\n",
    "3. Use probabilistic programming for parameter estimation;\n",
    "4. Use probabilistic programming for hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Bayes Theorem to Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we flip a biased coin several times and we want to estimate the probability of heads from the number of heads we saw. Statistical intuition tells us that our best estimate of $p(heads)=$ number of heads divided by total number of flips.\n",
    "\n",
    "However, \n",
    "\n",
    "1. It doesn't tell us how certain we can be of that estimate and\n",
    "2. This type of intuition doesn't extend to even slightly more complex examples.\n",
    "\n",
    "Bayesian inference helps us here. We can calculate the probability of a particular $p=p(H)$ given data $D$ by setting $A$ in Bayes Theorem equal to $p$ and $B$ equal to $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$P(p|D) = \\frac{P(D|p)P(p)}{P(D)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this equation, we call $P(p)$ the prior (distribution), $P(D|p)$ the likelihood and $P(p|D)$ the posterior (distribution). The intuition behind the nomenclature is as follows: the prior is the distribution containing our knowledge about $p$ prior to the introduction of the data $D$ & the posterior is the distribution containing our knowledge about $p$ after considering the data $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that we're _overloading_ the term _probability_ here. In fact, we have 3 distinct usages of the word:\n",
    "- The probability $p$ of seeing a head when flipping a coin;\n",
    "- The resulting binomial probability distribution $P(D|p)$ of seeing the data $D$, given $p$;\n",
    "- The prior & posterior probability distributions of $p$, encoding our _uncertainty_ about the value of $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key concept:** We only need to know the posterior distribution $P(p|D)$ up to multiplication by a constant at the moment: this is because we really only care about the values of $P(p|D)$ relative to each other – for example, what is the most likely value of $p$? To answer such questions, we only need to know what $P(p|D)$ is proportional to, as a function of $p$. Thus we don’t currently need to worry about the term $P(D)$. In fact,\n",
    "\n",
    "$$P(p|D) \\propto P(D|p)P(p) $$\n",
    "\n",
    "**Note:** What is the prior? Really, what do we know about $p$ before we see any data? Well, as it is a probability, we know that $0\\leq p \\leq1$. If we haven’t flipped any coins yet, we don’t know much else: so it seems logical that all values of $p$ within this interval are equally likely, i.e., $P(p)=1$, for $0\\leq p \\leq1$. This is known as an uninformative prior because it contains little information (there are other uninformative priors we may use in this situation, such as the Jeffreys prior, to be discussed later). People who like to hate on Bayesian inference tend to claim that the need to choose a prior makes Bayesian methods somewhat arbitrary, but as we’ll now see, if you have enough data, the likelihood dominates over the prior and the latter doesn’t matter so much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Essential remark:** we get the whole distribution of $P(p|D)$, not merely a point estimate plus errors bars, such as [95% confidence intervals](http://andrewgelman.com/2018/07/04/4th-july-lets-declare-independence-95/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayesian parameter estimation I: flip those coins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some coin flips and try to estimate $p(H)$. Two notes:\n",
    "- given data $D$ consisting of $n$ coin tosses & $k$ heads, the likelihood function is given by $L:=P(D|p) \\propto p^k(1-p)^k$;\n",
    "- given a uniform prior, the posterior is proportional to the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior(p=0.6, N=0):\n",
    "    \"\"\"Plot the posterior given a uniform prior; Bernoulli trials\n",
    "    with probability p; sample size N\"\"\"\n",
    "    np.random.seed(42)\n",
    "    # Flip coins \n",
    "    n_successes = np.random.binomial(N, p)\n",
    "    # X-axis for PDF\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    #prior\n",
    "    prior = 1\n",
    "    posterior = x**n_successes*(1-x)**(N-n_successes)*prior\n",
    "    posterior /= np.max(posterior)  # so that peak always at 1\n",
    "    plt.plot(x, posterior)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now use the great ipywidget interact to check out the posterior as you generate more and more data (you can also vary $p$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_posterior, p=(0, 1, 0.01), N=(0, 1500));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes for discussion:**\n",
    "\n",
    "* as you generate more and more data, your posterior gets narrower, i.e. you get more and more certain of your estimate.\n",
    "* you need more data to be certain of your estimate when $p=0.5$, as opposed to when $p=0$ or $p=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The choice of the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that we needed to choose a prior and that, in the small to medium data limit, this choice can affect the posterior. We'll briefly introduce several types of priors and then you'll use one of them for the example above to see the effect of the prior:\n",
    "\n",
    "- **Informative priors** express specific, definite information about a variable, for example, if we got a coin from the mint, we may use an informative prior with a peak at $p=0.5$ and small variance. \n",
    "- **Weakly informative priors** express partial information about a variable, such as a peak at $p=0.5$ (if we have no reason to believe the coin is biased), with a larger variance.\n",
    "- **Uninformative priors** express no information about a variable, except what we know for sure, such as knowing that $0\\leq p \\leq1$.\n",
    "\n",
    "Now you may think that the _uniform distribution_ is uninformative, however, what if I am thinking about this question in terms of the probability $p$ and Eric Ma is thinking about it in terms of the _odds ratio_ $r=\\frac{p}{1-p}$? Eric rightly feels that he has no prior knowledge as to what this $r$ is and thus chooses the uniform prior on $r$.\n",
    "\n",
    "With a bit of algebra (transformation of variables), we can show that choosing the uniform prior on $p$ amounts to choosing a decidedly non-uniform prior on $r$ and vice versa. So Eric and I have actually chosen different priors, using the same philosophy. How do we avoid this happening? Enter the **Jeffreys prior**, which is an uninformative prior that solves this problem. You can read more about the Jeffreys prior [here](https://en.wikipedia.org/wiki/Jeffreys_prior) & in your favourite Bayesian text book (Sivia gives a nice treatment). \n",
    "\n",
    "In the binomial (coin flip) case, the Jeffreys prior is given by $P(p) = \\frac{1}{\\sqrt{p(1-p)}}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create an interactive plot like the one above, except that it has two posteriors on it: one for the uniform prior, another for the Jeffries prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def plot_posteriors(p=0.6, N=0):\n",
    "    np.random.seed(42)\n",
    "    n_successes = np.random.binomial(N, p)\n",
    "    x = np.linspace(0.01, 0.99, 100)\n",
    "    posterior1 = x**n_successes*(1-x)**(N-n_successes)  # w/ uniform prior\n",
    "    posterior1 /= np.max(posterior1)  # so that peak always at 1\n",
    "    plt.plot(x, posterior1, label='Uniform prior')\n",
    "    jp = np.sqrt(x*(1-x))**(-1)  # Jeffreys prior\n",
    "    posterior2 = posterior1*jp  # w/ Jeffreys prior\n",
    "    posterior2 /= np.max(posterior2)  # so that peak always at 1 (not quite correct to do; see below)\n",
    "    plt.plot(x, posterior2, label='Jeffreys prior')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_posteriors, p=(0, 1, 0.01), N=(0, 100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What happens to the posteriors as you generate more and more data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayesian parameter estimation using PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation I: click-through rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click-through rates\n",
    "p_a = 0.15\n",
    "N = 150\n",
    "n_successes_a = np.sum(np.random.binomial(N,p_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model of p_a\n",
    "with pm.Model() as Model:\n",
    "    # Prior on p\n",
    "    prob = pm.Uniform('p')\n",
    "    # Binomial Likelihood\n",
    "    y = pm.Binomial('y', n=N, p=prob, observed=n_successes_a)\n",
    "\n",
    "with Model:\n",
    "    samples = pm.sample(1000, njobs=1)\n",
    "\n",
    "pm.plot_posterior(samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on: Parameter estimation II -- the mean of a population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll calculate the  posterior mean beak depth of Galapagos finches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and view head of data\n",
    "df_12 = pd.read_csv('../data/finch_beaks_2012.csv')\n",
    "df_fortis = df_12.loc[df_12['species'] == 'fortis']\n",
    "df_scandens = df_12.loc[df_12['species'] == 'scandens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    \"\"\"\n",
    "    The priors for each group.\n",
    "    \"\"\"\n",
    "    μ_1 = pm.Normal('μ_1', mu=10, sd=5)\n",
    "    σ_1 = pm.Lognormal('σ_1', 0, 10)\n",
    "    # Gaussian Likelihood\n",
    "    y_1 = pm.Normal('y_1', mu=μ_1, sd=σ_1, observed=df_fortis['blength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bust it out & sample\n",
    "with model:\n",
    "    samples = pm.sample(2000, njobs=1)\n",
    "    \n",
    "pm.plot_posterior(samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bayesian Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Hypothesis testing I: A/B tests on click through rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have a website and want to redesign the layout (_A_) and test whether the new layout (_B_) results in a higher click through rate. When people come to our website we randomly show them layout _A_ or _B_ and see how many people click through for each. First let's generate the data we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click-through rates\n",
    "p_a = 0.15\n",
    "p_b = 0.20\n",
    "N = 1000\n",
    "n_successes_a = np.sum(np.random.uniform(size=N) <= p_a)\n",
    "n_successes_b = np.sum(np.random.uniform(size=N) <= p_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as Model:\n",
    "    # Prior on p\n",
    "    prob_a = pm.Uniform('p_a')\n",
    "    prob_b = pm.Uniform('p_b')\n",
    "    # Binomial Likelihood\n",
    "    y_a = pm.Binomial('y_a', n=N, p=prob_a, observed=n_successes_a)\n",
    "    y_b = pm.Binomial('y_b', n=N, p=prob_b, observed=n_successes_b)\n",
    "    diff_clicks = pm.Deterministic('diff_clicks', prob_a-prob_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Model:\n",
    "    samples = pm.sample(1000, njobs=1)\n",
    "pm.plot_posterior(samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on: Bayesian Hypothesis testing II -- beak lengths difference between species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Determine whether the mean beak length of the Galapogas finches differs between species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Priors for means and variances\n",
    "    μ_1 = pm.Normal('μ_1', mu=10, sd=5)\n",
    "    σ_1 = pm.Lognormal('σ_1', 0, 10)\n",
    "    μ_2 = pm.Normal('μ_2', mu=10, sd=5)\n",
    "    σ_2 = pm.Lognormal('σ_2', 0, 10)\n",
    "    # Gaussian Likelihoods\n",
    "    y_1 = pm.Normal('y_1', mu=μ_1, sd=σ_1, observed=df_fortis['blength'])\n",
    "    y_2 = pm.Normal('y_2', mu=μ_2, sd=σ_2, observed=df_scandens['blength'])\n",
    "    # Calculate the effect size and its uncertainty.\n",
    "    diff_means = pm.Deterministic('diff_means', μ_1 - μ_2)\n",
    "    pooled_sd = pm.Deterministic('pooled_sd', \n",
    "                                 np.sqrt(np.power(σ_1, 2) + \n",
    "                                         np.power(σ_2, 2)) / 2)\n",
    "    effect_size = pm.Deterministic('effect_size', \n",
    "                                   diff_means / pooled_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bust it out & sample\n",
    "with model:\n",
    "    samples = pm.sample(2000, njobs=1)\n",
    "pm.plot_posterior(samples, varnames=['μ_1', 'μ_2', 'diff_means', 'effect_size']);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
